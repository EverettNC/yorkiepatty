# /speech/transcriber.py
# Hybrid VOSK + AlphaVox Speech Recognition + Derek Brain Integration
# Adds "Hey Derek" wake word mode and Voice Activity Detection (VAD)

import os
import json
import queue
import sounddevice as sd
import vosk
import sys
import webrtcvad
import time

# -------------------------------------------------------------
# Integrate with Derek‚Äôs Brain + Speech Output
# -------------------------------------------------------------
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from brain import derek
from tts_bridge import speak_response

# -------------------------------------------------------------
# Configuration
# -------------------------------------------------------------
DEFAULT_MODEL_PATH = "./speech/vosk-model-small-en-us-0.15"
MODEL_PATH = os.getenv("VOSK_MODEL_PATH", DEFAULT_MODEL_PATH)

if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(
        f"‚ùå VOSK model not found at {MODEL_PATH}\n"
        f"‚Üí Please download 'vosk-model-small-en-us-0.15' into ./speech/."
    )

print(f"‚úÖ Using Vosk model at: {MODEL_PATH}")
model = vosk.Model(MODEL_PATH)

CUSTOM_VOCAB_PATH = "./speech/vocab-alpha.json"
custom_vocab = {"terms": []}
if os.path.exists(CUSTOM_VOCAB_PATH):
    with open(CUSTOM_VOCAB_PATH, "r") as f:
        custom_vocab = json.load(f)

SAMPLE_RATE = 16000
FRAME_DURATION = 30
FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION / 1000) * 2

# -------------------------------------------------------------
# Audio + VAD setup
# -------------------------------------------------------------
q = queue.Queue()
vad = webrtcvad.Vad(2)

def callback(indata, frames, time_info, status):
    if status:
        print(f"[Audio Warning] {status}", flush=True)
    q.put(bytes(indata))

# -------------------------------------------------------------
# Wake Word Detection
# -------------------------------------------------------------
WAKE_WORDS = ["hey derek", "okay derek", "hi derek"]

def detect_wake_word(text: str) -> bool:
    text = text.lower().strip()
    return any(ww in text for ww in WAKE_WORDS)

# -------------------------------------------------------------
# Speech Recognition Modes
# -------------------------------------------------------------
def passive_listen():
    """Idle mode ‚Äî listens for 'Hey Derek' without responding to everything."""
    print("üëÇ Passive listening... (Say 'Hey Derek' to wake me)")
    rec = vosk.KaldiRecognizer(model, SAMPLE_RATE)
    buffer = b""

    with sd.RawInputStream(
        samplerate=SAMPLE_RATE, blocksize=FRAME_SIZE, dtype="int16",
        channels=1, callback=callback
    ):
        while True:
            frame = q.get()
            buffer += frame

            if len(buffer) >= FRAME_SIZE:
                chunk = buffer[:FRAME_SIZE]
                buffer = buffer[FRAME_SIZE:]

                if vad.is_speech(chunk, SAMPLE_RATE):
                    rec.AcceptWaveform(chunk)
                    result = json.loads(rec.Result())
                    text = result.get("text", "")
                    if text:
                        sys.stdout.write(f"\rüó£Ô∏è Heard: {text}     ")
                        sys.stdout.flush()

                        if detect_wake_word(text):
                            print("\nüöÄ Wake word detected! Derek is listening...")
                            speak_response("Yes?")
                            active_listen()
                            print("üëÇ Back to passive listening...\n")
                            rec = vosk.KaldiRecognizer(model, SAMPLE_RATE)

# -------------------------------------------------------------
def active_listen():
    """Active mode ‚Äî listens for full sentences until silence is detected."""
    print("üé§ Active listening mode. Speak your command...")

    rec = vosk.KaldiRecognizer(model, SAMPLE_RATE)
    silence_counter = 0
    speaking = False
    buffer = b""

    with sd.RawInputStream(
        samplerate=SAMPLE_RATE, blocksize=FRAME_SIZE, dtype="int16",
        channels=1, callback=callback
    ):
        while True:
            frame = q.get()
            buffer += frame

            if len(buffer) >= FRAME_SIZE:
                chunk = buffer[:FRAME_SIZE]
                buffer = buffer[FRAME_SIZE:]
                speech_detected = vad.is_speech(chunk, SAMPLE_RATE)

                if speech_detected:
                    speaking = True
                    silence_counter = 0
                    rec.AcceptWaveform(chunk)
                else:
                    if speaking:
                        silence_counter += 1
                        # about 0.8s silence ends capture
                        if silence_counter > int(800 / FRAME_DURATION):
                            speaking = False
                            process_audio(rec)
                            return  # Go back to passive listening

# -------------------------------------------------------------
def process_audio(rec):
    """Send recognized speech to Derek‚Äôs brain and speak back."""
    result = json.loads(rec.FinalResult())
    text = result.get("text", "").strip()
    if not text:
        print("‚ö†Ô∏è No speech detected.")
        return

    corrected = postprocess(text)
    print(f"\nüß† You said: {corrected}")

    try:
        response = derek.think(corrected)
        reply = response.get("response", "I'm here.")
        print(f"ü§ñ Derek: {reply}\n")
        speak_response(reply)
    except Exception as e:
        print(f"‚ö†Ô∏è Derek response error: {e}")

# -------------------------------------------------------------
def postprocess(text):
    """Clean up transcription and apply vocab corrections."""
    corrections = {"ice cream": "I scream", "help me now": "emergency help"}
    for wrong, right in corrections.items():
        text = text.replace(wrong, right)
    for term in custom_vocab.get("terms", []):
        alias = term.get("alias")
        canonical = term.get("canonical")
        if alias and canonical and alias in text:
            text = text.replace(alias, canonical)
    return text

# -------------------------------------------------------------
if __name__ == "__main__":
    try:
        passive_listen()
    except KeyboardInterrupt:
        print("\nüõë Exiting voice system.")

